# LLM Injection

LLMs can't distinguish the operations they are tasked with from the data that they are operating on. Many of the most exploited vulnerabilities in traditional software occur when programmers accidentally allow the data a program operates on to be interpreted as instructions. Think of SQL injection attacks, or all the damage caused by people running JavaScript's eval on user input. 

LLMs are fundamentally incapable of distinguishing between  program and data. So when you are building an based LLM system, you are always operating with something analaguous to an unpatched SQL injection vulnerability.
